{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "from prune_layer import *\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# import time\n",
    "# Custom Libraries\n",
    "import utils\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Initialization\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_percentage_nonzero(q = 10):\n",
    "    global model \n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            m.prune_by_percentage(q = q)\n",
    "        if isinstance(m, PruneLinear):\n",
    "            m.prune_by_percentage(q = q)\n",
    "            \n",
    "def mask_weights(mask_data = True): \n",
    "    global model \n",
    "    if mask_data:\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, PrunedConv):\n",
    "                m.conv.weight.data.mul_(m.mask)\n",
    "            if isinstance(m, PruneLinear):\n",
    "                m.linear.weight.data.mul_(m.mask)\n",
    "    else:\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, PrunedConv):\n",
    "                m.conv.weight.grad.mul_(m.mask)\n",
    "            if isinstance(m, PruneLinear):\n",
    "                m.linear.weight.grad.mul_(m.mask)\n",
    "            \n",
    "def initialize_weights(initial_state_dict):\n",
    "    global model \n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            m.conv.weight.data = m.mask*initial_state_dict[n + '.conv.weight']\n",
    "            m.conv.weight.bias = initial_state_dict[n + '.conv.bias']\n",
    "        if isinstance(m, PruneLinear):\n",
    "            m.linear.weight.data = m.mask*initial_state_dict[n + '.linear.weight']\n",
    "            m.linear.weight.bias =initial_state_dict[n + '.linear.bias']\n",
    "            \n",
    "def reintilize_weights(weight_init):\n",
    "    global model \n",
    "    model.apply(weight_init)\n",
    "    mask_weights()\n",
    "\n",
    "\n",
    "# Function for Training\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    model.train()\n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask_weights()#Mask data into zero\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "#         mask_weights(False) #Mask gradients of weights to zero\n",
    "        optimizer.step()\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "# Function for Testing\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "def get_mask_all():\n",
    "    global model\n",
    "    mask = []\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            mask += [m.mask.cpu().numpy()]\n",
    "        if isinstance(m, PruneLinear):\n",
    "            mask += [m.mask.cpu().numpy()]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ITE=0\n",
    "    reinit = True if args.prune_type==\"reinit\" else False\n",
    "\n",
    "    #Data Loader\n",
    "    transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    if args.dataset == \"mnist\":\n",
    "        traindataset = datasets.MNIST('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.MNIST('~/work/data/Xian', train=False, transform=transform)\n",
    "        from archs.mnist import  LeNet5, fc1, vgg, resnet,AlexNet\n",
    "    # If you want to add extra datasets paste here\n",
    "    else:\n",
    "        print(\"\\nWrong Dataset choice \\n\")\n",
    "        exit()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(traindataset, batch_size=args.batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "    test_loader = torch.utils.data.DataLoader(testdataset, batch_size=args.batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
    "\n",
    "    # Importing Network Architecture\n",
    "    global model\n",
    "    if args.arch_type == \"fc1\":\n",
    "        model = fc1.fc1().to(device)\n",
    "    elif args.arch_type == \"lenet5\":\n",
    "        model = LeNet5.LeNet5().to(device)\n",
    "    else:\n",
    "        print(\"\\nWrong Model choice\\n\")\n",
    "        exit()\n",
    "\n",
    "    model.apply(weight_init)\n",
    "    # Copying and Saving Initial State\n",
    "    initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "    utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "    torch.save(model, f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/initial_state_dict_{args.prune_type}.pth.tar\")\n",
    "\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss() # Default was F.nll_loss\n",
    "\n",
    "    # Layer Looper\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(name, param.size())\n",
    "\n",
    "\n",
    "    # Pruning\n",
    "    # NOTE First Pruning Iteration is of No Compression\n",
    "    bestacc = 0.0\n",
    "    best_accuracy = 0\n",
    "    ITERATION = args.prune_iterations\n",
    "    comp = np.zeros(ITERATION,float)\n",
    "    bestacc = np.zeros(ITERATION,float)\n",
    "    step = 0\n",
    "    all_loss = np.zeros(args.end_iter,float)\n",
    "    all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "    for _ite in range(args.start_iter, ITERATION):\n",
    "        if not _ite == 0:\n",
    "            prune_percentage_nonzero(q = 10)\n",
    "            if reinit:\n",
    "                reintilize_weights(weight_init)\n",
    "            else:\n",
    "                initialize_weights(initial_state_dict)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "        print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "        # Print the table of Nonzeros in each layer\n",
    "        comp1 = utils.print_nonzeros(model)\n",
    "        comp[_ite] = comp1\n",
    "        pbar = tqdm(range(args.end_iter))\n",
    "\n",
    "        for iter_ in pbar:\n",
    "\n",
    "            # Frequency for Testing\n",
    "            if iter_ % args.valid_freq == 0:\n",
    "                accuracy = test(model, test_loader, criterion)\n",
    "\n",
    "                # Save Weights\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "                    torch.save(model,f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/{_ite}_model_{args.prune_type}.pth.tar\")\n",
    "\n",
    "            # Training\n",
    "            loss = train(model, train_loader, optimizer, criterion)\n",
    "            all_loss[iter_] = loss\n",
    "            all_accuracy[iter_] = accuracy\n",
    "\n",
    "            # Frequency for Printing Accuracy and Loss\n",
    "            if iter_ % args.print_freq == 0:\n",
    "                pbar.set_description(\n",
    "                    f'Train Epoch: {iter_}/{args.end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "        writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "        bestacc[_ite]=best_accuracy\n",
    "\n",
    "        # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "        #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "        #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "        plt.plot(np.arange(1,(args.end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "        plt.plot(np.arange(1,(args.end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "        plt.title(f\"Loss Vs Accuracy Vs Iterations ({args.dataset},{args.arch_type})\") \n",
    "        plt.xlabel(\"Iterations\") \n",
    "        plt.ylabel(\"Loss and Accuracy\") \n",
    "        plt.legend() \n",
    "        plt.grid(color=\"gray\") \n",
    "        utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "        plt.close()\n",
    "\n",
    "        # Dump Plot values\n",
    "        utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        all_loss.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_loss_{comp1}.dat\")\n",
    "        all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_accuracy_{comp1}.dat\")\n",
    "\n",
    "        # Dumping mask\n",
    "        mask = get_mask_all()\n",
    "        utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        with open(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "            pickle.dump(mask, fp)\n",
    "\n",
    "        # Making variables into 0\n",
    "        best_accuracy = 0\n",
    "        all_loss = np.zeros(args.end_iter,float)\n",
    "        all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "    # Dumping Values for Plotting\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    comp.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_compression.dat\")\n",
    "    bestacc.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_bestaccuracy.dat\")\n",
    "\n",
    "    # Plotting\n",
    "    a = np.arange(args.prune_iterations)\n",
    "    plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "    plt.title(f\"Test Accuracy vs Unpruned Weights Percentage ({args.dataset},{args.arch_type})\") \n",
    "    plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "    plt.ylabel(\"test accuracy\") \n",
    "    plt.xticks(a, comp, rotation =\"vertical\") \n",
    "    plt.ylim(0,100)\n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_AccuracyVsWeights.png\", dpi=1200) \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [0:0/2]: ---\n",
      "conv1.conv.weight    | nonzeros =     576 /     576 (100.00%) | total_pruned =       0 | shape = (64, 1, 3, 3)\n",
      "conv1.conv.bias      | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_1.weight        | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_1.bias          | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "conv2.conv.weight    | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "conv2.conv.bias      | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_2.weight        | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_2.bias          | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "fc1.linear.weight    | nonzeros = 3211263 / 3211264 (100.00%) | total_pruned =       1 | shape = (256, 12544)\n",
      "fc1.linear.bias      | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_1.weight        | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_1.bias          | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "fc2.linear.weight    | nonzeros =   65536 /   65536 (100.00%) | total_pruned =       0 | shape = (256, 256)\n",
      "fc2.linear.bias      | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_2.weight        | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_2.bias          | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "fc3.linear.weight    | nonzeros =    2560 /    2560 (100.00%) | total_pruned =       0 | shape = (10, 256)\n",
      "fc3.linear.bias      | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 3318089, pruned : 641, total: 3318730, Compression rate :       1.00x  (  0.02% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/2 Loss: 0.031546 Accuracy: 98.35% Best Accuracy: 98.35%: 100%|██████████| 2/2 [00:46<00:00, 23.14s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [0:1/2]: ---\n",
      "conv1.conv.weight    | nonzeros =     518 /     576 ( 89.93%) | total_pruned =      58 | shape = (64, 1, 3, 3)\n",
      "conv1.conv.bias      | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_1.weight        | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_1.bias          | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "conv2.conv.weight    | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "conv2.conv.bias      | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_2.weight        | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_2.bias          | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "fc1.linear.weight    | nonzeros = 2890137 / 3211264 ( 90.00%) | total_pruned =  321127 | shape = (256, 12544)\n",
      "fc1.linear.bias      | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_1.weight        | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_1.bias          | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "fc2.linear.weight    | nonzeros =   58982 /   65536 ( 90.00%) | total_pruned =    6554 | shape = (256, 256)\n",
      "fc2.linear.bias      | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_2.weight        | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_2.bias          | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "fc3.linear.weight    | nonzeros =    2304 /    2560 ( 90.00%) | total_pruned =     256 | shape = (10, 256)\n",
      "fc3.linear.bias      | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 2987048, pruned : 331682, total: 3318730, Compression rate :       1.11x  (  9.99% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/2 Loss: 0.021028 Accuracy: 98.65% Best Accuracy: 98.65%: 100%|██████████| 2/2 [00:45<00:00, 22.86s/it]\n"
     ]
    }
   ],
   "source": [
    "class argument:\n",
    "    def __init__(self, lr=1.2e-3,batch_size = 128,start_iter = 0,end_iter = 100,print_freq = 1,\n",
    "                 valid_freq = 1,resume = \"store_true\",prune_type= \"lt\",gpu = \"0\",\n",
    "                 dataset = \"mnist\" ,arch_type = \"fc1\",prune_percent  = 10,prune_iterations = 35):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.start_iter = start_iter\n",
    "        self.end_iter = end_iter\n",
    "        self.print_freq = print_freq\n",
    "        self.valid_freq = valid_freq\n",
    "        self.resume = resume\n",
    "        self.prune_type = prune_type #reinit\n",
    "        self.gpu = gpu\n",
    "        self.dataset = dataset #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "        self.arch_type = arch_type # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "        self.prune_percent  = prune_percent \n",
    "        self.prune_iterations = prune_iterations \n",
    "args = argument(end_iter = 2,arch_type =\"lenet5\",prune_percent  = 20,prune_iterations = 2)\n",
    "\n",
    "train_main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get_mask\n",
    "# global mask\n",
    "# step = 0\n",
    "# for name, param in model.named_parameters(): \n",
    "#     if 'weight' in name:\n",
    "#         step = step + 1\n",
    "# mask = [None]* step \n",
    "# step = 0\n",
    "# for name, param in model.named_parameters(): \n",
    "#     if 'weight' in name:\n",
    "#         print(name)\n",
    "#         tensor = param.data.cpu().numpy()\n",
    "#         mask[step] = np.ones_like(tensor)\n",
    "#         step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_loader, optimizer, criterion):\n",
    "#     EPS = 1e-6\n",
    "#     model.train()\n",
    "#     s = time.time()\n",
    "#     for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         mask_weights()#Mask data into zero\n",
    "#         imgs, targets = imgs.to(device), targets.to(device)\n",
    "#         output = model(imgs)\n",
    "#         train_loss = criterion(output, targets)\n",
    "#         train_loss.backward()\n",
    "# #         mask_weights(False) #Mask gradients of weights to zero\n",
    "#         optimizer.step()\n",
    "#     print(time.time()-s)\n",
    "#     return train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dumps/lt/fc1/mnist/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
