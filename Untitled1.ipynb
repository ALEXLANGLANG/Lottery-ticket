{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [0:0/35]: ---\n",
      "conv1.conv.weight    | nonzeros =     576 /     576 (100.00%) | total_pruned =       0 | shape = (64, 1, 3, 3)\n",
      "conv1.conv.bias      | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_1.weight        | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_1.bias          | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "conv2.conv.weight    | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "conv2.conv.bias      | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_2.weight        | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "BN2d_2.bias          | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "fc1.linear.weight    | nonzeros = 3211264 / 3211264 (100.00%) | total_pruned =       0 | shape = (256, 12544)\n",
      "fc1.linear.bias      | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_1.weight        | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_1.bias          | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "fc2.linear.weight    | nonzeros =   65536 /   65536 (100.00%) | total_pruned =       0 | shape = (256, 256)\n",
      "fc2.linear.bias      | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_2.weight        | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "BN1d_2.bias          | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "fc3.linear.weight    | nonzeros =    2560 /    2560 (100.00%) | total_pruned =       0 | shape = (10, 256)\n",
      "fc3.linear.bias      | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 3318090, pruned : 640, total: 3318730, Compression rate :       1.00x  (  0.02% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0/1 Loss: 0.013505 Accuracy: 11.72% Best Accuracy: 11.72%: 100%|██████████| 1/1 [00:15<00:00, 15.15s/it]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:220: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-85b3ea8e9bc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_mask_{comp1}.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# Making variables into 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "from prune_layer import *\n",
    "writer = SummaryWriter()\n",
    "\n",
    "import time\n",
    "# Custom Libraries\n",
    "import utils\n",
    "class argument:\n",
    "    def __init__(self, lr=1.2e-3,batch_size = 128,start_iter = 0,end_iter = 100,print_freq = 1,\n",
    "                 valid_freq = 1,resume = \"store_true\",prune_type= \"lt\",gpu = \"0\",\n",
    "                 dataset = \"mnist\" ,arch_type = \"fc1\",prune_percent  = 10,prune_iterations = 35):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.start_iter = start_iter\n",
    "        self.end_iter = end_iter\n",
    "        self.print_freq = print_freq\n",
    "        self.valid_freq = valid_freq\n",
    "        self.resume = resume\n",
    "        self.prune_type = prune_type #reinit\n",
    "        self.gpu = gpu\n",
    "        self.dataset = dataset #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "        self.arch_type = arch_type # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "        self.prune_percent  = prune_percent \n",
    "        self.prune_iterations = prune_iterations \n",
    "def prune_percentage_nonzero(q = 10):\n",
    "    global model \n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            m.prune_by_percentage(q = q)\n",
    "        if isinstance(m, PruneLinear):\n",
    "            m.prune_by_percentage(q = q)\n",
    "            \n",
    "def mask_weights(mask_data = True): \n",
    "    global model \n",
    "    if mask_data:\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, PrunedConv):\n",
    "                m.conv.weight.data.mul_(m.mask)\n",
    "            if isinstance(m, PruneLinear):\n",
    "                m.linear.weight.data.mul_(m.mask)\n",
    "    else:\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, PrunedConv):\n",
    "                m.conv.weight.grad.mul_(m.mask)\n",
    "            if isinstance(m, PruneLinear):\n",
    "                m.linear.weight.grad.mul_(m.mask)\n",
    "            \n",
    "def initialize_weights(initial_state_dict):\n",
    "    global model \n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            m.conv.weight.data = m.mask*initial_state_dict[n + '.conv.weight']\n",
    "            m.conv.weight.bias = initial_state_dict[n + '.conv.bias']\n",
    "        if isinstance(m, PruneLinear):\n",
    "            m.linear.weight.data = m.mask*initial_state_dict[n + '.linear.weight']\n",
    "            m.linear.weight.bias =initial_state_dict[n + '.linear.bias']\n",
    "            \n",
    "def reintilize_weights(weight_init):\n",
    "    global model \n",
    "    model.apply(weight_init)\n",
    "    mask_weights()\n",
    "\n",
    "\n",
    "# Function for Training\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    model.train()\n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask_weights()#Mask data into zero\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "#         mask_weights(False) #Mask gradients of weights to zero\n",
    "        optimizer.step()\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "# Function for Testing\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ITE=0\n",
    "\n",
    "\n",
    "args = argument(end_iter = 1,arch_type =\"lenet5\")\n",
    "reinit = True if args.prune_type==\"reinit\" else False\n",
    "\n",
    "#Data Loader\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "if args.dataset == \"mnist\":\n",
    "    traindataset = datasets.MNIST('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "    testdataset = datasets.MNIST('~/work/data/Xian', train=False, transform=transform)\n",
    "    from archs.mnist import  LeNet5, fc1, vgg, resnet,AlexNet\n",
    "# If you want to add extra datasets paste here\n",
    "else:\n",
    "    print(\"\\nWrong Dataset choice \\n\")\n",
    "    exit()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(traindataset, batch_size=args.batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size=args.batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
    "\n",
    "# Importing Network Architecture\n",
    "global model\n",
    "if args.arch_type == \"fc1\":\n",
    "    model = fc1.fc1().to(device)\n",
    "elif args.arch_type == \"lenet5\":\n",
    "    model = LeNet5.LeNet5().to(device)\n",
    "else:\n",
    "    print(\"\\nWrong Model choice\\n\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Copying and Saving Initial State\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "torch.save(model, f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/initial_state_dict_{args.prune_type}.pth.tar\")\n",
    "\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss() # Default was F.nll_loss\n",
    "\n",
    "# Layer Looper\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.size())\n",
    "    \n",
    "    \n",
    "# Pruning\n",
    "# NOTE First Pruning Iteration is of No Compression\n",
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "ITERATION = args.prune_iterations\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(args.end_iter,float)\n",
    "all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "for _ite in range(args.start_iter, ITERATION):\n",
    "    if not _ite == 0:\n",
    "        prune_percentage_nonzero(q = 10)\n",
    "        if reinit:\n",
    "            reintilize_weights(weight_init)\n",
    "        else:\n",
    "            initialize_weights(initial_state_dict)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "        \n",
    "    # Print the table of Nonzeros in each layer\n",
    "    comp1 = utils.print_nonzeros(model)\n",
    "    comp[_ite] = comp1\n",
    "    pbar = tqdm(range(args.end_iter))\n",
    "\n",
    "    for iter_ in pbar:\n",
    "\n",
    "        # Frequency for Testing\n",
    "        if iter_ % args.valid_freq == 0:\n",
    "            accuracy = test(model, test_loader, criterion)\n",
    "\n",
    "            # Save Weights\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "                torch.save(model,f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/{_ite}_model_{args.prune_type}.pth.tar\")\n",
    "\n",
    "        # Training\n",
    "        loss = train(model, train_loader, optimizer, criterion)\n",
    "        all_loss[iter_] = loss\n",
    "        all_accuracy[iter_] = accuracy\n",
    "\n",
    "        # Frequency for Printing Accuracy and Loss\n",
    "        if iter_ % args.print_freq == 0:\n",
    "            pbar.set_description(\n",
    "                f'Train Epoch: {iter_}/{args.end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "    writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "    bestacc[_ite]=best_accuracy\n",
    "\n",
    "    # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "    #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "    #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "    plt.plot(np.arange(1,(args.end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "    plt.plot(np.arange(1,(args.end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "    plt.title(f\"Loss Vs Accuracy Vs Iterations ({args.dataset},{args.arch_type})\") \n",
    "    plt.xlabel(\"Iterations\") \n",
    "    plt.ylabel(\"Loss and Accuracy\") \n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "    plt.close()\n",
    "\n",
    "    # Dump Plot values\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    all_loss.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_loss_{comp1}.dat\")\n",
    "    all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_accuracy_{comp1}.dat\")\n",
    "\n",
    "    # Dumping mask\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    with open(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(mask, fp)\n",
    "\n",
    "    # Making variables into 0\n",
    "    best_accuracy = 0\n",
    "    all_loss = np.zeros(args.end_iter,float)\n",
    "    all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "# Dumping Values for Plotting\n",
    "utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "comp.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_compression.dat\")\n",
    "bestacc.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_bestaccuracy.dat\")\n",
    "\n",
    "# Plotting\n",
    "a = np.arange(args.prune_iterations)\n",
    "plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "plt.title(f\"Test Accuracy vs Unpruned Weights Percentage ({args.dataset},{args.arch_type})\") \n",
    "plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "plt.ylabel(\"test accuracy\") \n",
    "plt.xticks(a, comp, rotation =\"vertical\") \n",
    "plt.ylim(0,100)\n",
    "plt.legend() \n",
    "plt.grid(color=\"gray\") \n",
    "utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_AccuracyVsWeights.png\", dpi=1200) \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
