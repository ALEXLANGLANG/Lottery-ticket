{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                             [--start_iter START_ITER] [--end_iter END_ITER]\n",
      "                             [--print_freq PRINT_FREQ]\n",
      "                             [--valid_freq VALID_FREQ] [--resume]\n",
      "                             [--prune_type PRUNE_TYPE] [--gpu GPU]\n",
      "                             [--dataset DATASET] [--arch_type ARCH_TYPE]\n",
      "                             [--prune_percent PRUNE_PERCENT]\n",
      "                             [--prune_iterations PRUNE_ITERATIONS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jovyan/.local/share/jupyter/runtime/kernel-989eab33-b36f-4f7e-b775-c07883dbba9e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import seaborn as sns\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "\n",
    "# Custom Libraries\n",
    "import utils\n",
    "\n",
    "# Tensorboard initialization\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Plotting Style\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Main\n",
    "def main(args, ITE=0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    reinit = True if args.prune_type==\"reinit\" else False\n",
    "\n",
    "    # Data Loader\n",
    "    transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    if args.dataset == \"mnist\":\n",
    "        traindataset = datasets.MNIST('../data', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "        from archs.mnist import AlexNet, LeNet5, fc1, vgg, resnet\n",
    "\n",
    "    elif args.dataset == \"cifar10\":\n",
    "        traindataset = datasets.CIFAR10('../data', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.CIFAR10('../data', train=False, transform=transform)      \n",
    "        from archs.cifar10 import AlexNet, LeNet5, fc1, vgg, resnet, densenet \n",
    "\n",
    "    elif args.dataset == \"fashionmnist\":\n",
    "        traindataset = datasets.FashionMNIST('../data', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.FashionMNIST('../data', train=False, transform=transform)\n",
    "        from archs.mnist import AlexNet, LeNet5, fc1, vgg, resnet \n",
    "\n",
    "    elif args.dataset == \"cifar100\":\n",
    "        traindataset = datasets.CIFAR100('../data', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.CIFAR100('../data', train=False, transform=transform)   \n",
    "        from archs.cifar100 import AlexNet, fc1, LeNet5, vgg, resnet  \n",
    "    \n",
    "    # If you want to add extra datasets paste here\n",
    "\n",
    "    else:\n",
    "        print(\"\\nWrong Dataset choice \\n\")\n",
    "        exit()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(traindataset, batch_size=args.batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "    #train_loader = cycle(train_loader)\n",
    "    test_loader = torch.utils.data.DataLoader(testdataset, batch_size=args.batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
    "    \n",
    "    # Importing Network Architecture\n",
    "    global model\n",
    "    if args.arch_type == \"fc1\":\n",
    "        model = fc1.fc1().to(device)\n",
    "    elif args.arch_type == \"lenet5\":\n",
    "        model = LeNet5.LeNet5().to(device)\n",
    "    elif args.arch_type == \"alexnet\":\n",
    "        model = AlexNet.AlexNet().to(device)\n",
    "    elif args.arch_type == \"vgg16\":\n",
    "        model = vgg.vgg16().to(device)  \n",
    "    elif args.arch_type == \"resnet18\":\n",
    "        model = resnet.resnet18().to(device)   \n",
    "    elif args.arch_type == \"densenet121\":\n",
    "        model = densenet.densenet121().to(device)   \n",
    "    # If you want to add extra model paste here\n",
    "    else:\n",
    "        print(\"\\nWrong Model choice\\n\")\n",
    "        exit()\n",
    "\n",
    "    # Weight Initialization\n",
    "    model.apply(weight_init)\n",
    "\n",
    "    # Copying and Saving Initial State\n",
    "    initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "    utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "    torch.save(model, f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/initial_state_dict_{args.prune_type}.pth.tar\")\n",
    "\n",
    "    # Making Initial Mask\n",
    "    make_mask(model)\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss() # Default was F.nll_loss\n",
    "\n",
    "    # Layer Looper\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.size())\n",
    "\n",
    "    # Pruning\n",
    "    # NOTE First Pruning Iteration is of No Compression\n",
    "    bestacc = 0.0\n",
    "    best_accuracy = 0\n",
    "    ITERATION = args.prune_iterations\n",
    "    comp = np.zeros(ITERATION,float)\n",
    "    bestacc = np.zeros(ITERATION,float)\n",
    "    step = 0\n",
    "    all_loss = np.zeros(args.end_iter,float)\n",
    "    all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "\n",
    "    for _ite in range(args.start_iter, ITERATION):\n",
    "        if not _ite == 0:\n",
    "            prune_by_percentile(args.prune_percent, resample=resample, reinit=reinit)\n",
    "            if reinit:\n",
    "                model.apply(weight_init)\n",
    "                #if args.arch_type == \"fc1\":\n",
    "                #    model = fc1.fc1().to(device)\n",
    "                #elif args.arch_type == \"lenet5\":\n",
    "                #    model = LeNet5.LeNet5().to(device)\n",
    "                #elif args.arch_type == \"alexnet\":\n",
    "                #    model = AlexNet.AlexNet().to(device)\n",
    "                #elif args.arch_type == \"vgg16\":\n",
    "                #    model = vgg.vgg16().to(device)  \n",
    "                #elif args.arch_type == \"resnet18\":\n",
    "                #    model = resnet.resnet18().to(device)   \n",
    "                #elif args.arch_type == \"densenet121\":\n",
    "                #    model = densenet.densenet121().to(device)   \n",
    "                #else:\n",
    "                #    print(\"\\nWrong Model choice\\n\")\n",
    "                #    exit()\n",
    "                step = 0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        weight_dev = param.device\n",
    "                        param.data = torch.from_numpy(param.data.cpu().numpy() * mask[step]).to(weight_dev)\n",
    "                        step = step + 1\n",
    "                step = 0\n",
    "            else:\n",
    "                original_initialization(mask, initial_state_dict)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "        print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "        # Print the table of Nonzeros in each layer\n",
    "        comp1 = utils.print_nonzeros(model)\n",
    "        comp[_ite] = comp1\n",
    "        pbar = tqdm(range(args.end_iter))\n",
    "\n",
    "        for iter_ in pbar:\n",
    "\n",
    "            # Frequency for Testing\n",
    "            if iter_ % args.valid_freq == 0:\n",
    "                accuracy = test(model, test_loader, criterion)\n",
    "\n",
    "                # Save Weights\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "                    torch.save(model,f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/{_ite}_model_{args.prune_type}.pth.tar\")\n",
    "\n",
    "            # Training\n",
    "            loss = train(model, train_loader, optimizer, criterion)\n",
    "            all_loss[iter_] = loss\n",
    "            all_accuracy[iter_] = accuracy\n",
    "            \n",
    "            # Frequency for Printing Accuracy and Loss\n",
    "            if iter_ % args.print_freq == 0:\n",
    "                pbar.set_description(\n",
    "                    f'Train Epoch: {iter_}/{args.end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "        writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "        bestacc[_ite]=best_accuracy\n",
    "\n",
    "        # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "        #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "        #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "        plt.plot(np.arange(1,(args.end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "        plt.plot(np.arange(1,(args.end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "        plt.title(f\"Loss Vs Accuracy Vs Iterations ({args.dataset},{args.arch_type})\") \n",
    "        plt.xlabel(\"Iterations\") \n",
    "        plt.ylabel(\"Loss and Accuracy\") \n",
    "        plt.legend() \n",
    "        plt.grid(color=\"gray\") \n",
    "        utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "        plt.close()\n",
    "\n",
    "        # Dump Plot values\n",
    "        utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        all_loss.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_loss_{comp1}.dat\")\n",
    "        all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_accuracy_{comp1}.dat\")\n",
    "        \n",
    "        # Dumping mask\n",
    "        utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        with open(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "            pickle.dump(mask, fp)\n",
    "        \n",
    "        # Making variables into 0\n",
    "        best_accuracy = 0\n",
    "        all_loss = np.zeros(args.end_iter,float)\n",
    "        all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "    # Dumping Values for Plotting\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    comp.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_compression.dat\")\n",
    "    bestacc.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_bestaccuracy.dat\")\n",
    "\n",
    "    # Plotting\n",
    "    a = np.arange(args.prune_iterations)\n",
    "    plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "    plt.title(f\"Test Accuracy vs Unpruned Weights Percentage ({args.dataset},{args.arch_type})\") \n",
    "    plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "    plt.ylabel(\"test accuracy\") \n",
    "    plt.xticks(a, comp, rotation =\"vertical\") \n",
    "    plt.ylim(0,100)\n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_AccuracyVsWeights.png\", dpi=1200) \n",
    "    plt.close()                    \n",
    "# Function for Training\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #imgs, targets = next(train_loader)\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Freezing Pruned weights by making their gradients Zero\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                tensor = p.data.cpu().numpy()\n",
    "                grad_tensor = p.grad.data.cpu().numpy()\n",
    "                grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "    return train_loss.item()\n",
    "\n",
    "# Function for Testing\n",
    "def test(model, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "# Prune by Percentile module\n",
    "def prune_by_percentile(percent, resample=False, reinit=False,**kwargs):\n",
    "        global step\n",
    "        global mask\n",
    "        global model\n",
    "\n",
    "        # Calculate percentile value\n",
    "        step = 0\n",
    "        for name, param in model.named_parameters():\n",
    "\n",
    "            # We do not prune bias term\n",
    "            if 'weight' in name:\n",
    "                tensor = param.data.cpu().numpy()\n",
    "                alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
    "                percentile_value = np.percentile(abs(alive), percent)\n",
    "\n",
    "                # Convert Tensors to numpy and calculate\n",
    "                weight_dev = param.device\n",
    "                new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])\n",
    "                \n",
    "                # Apply new weight and mask\n",
    "                param.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "                mask[step] = new_mask\n",
    "                step += 1\n",
    "        step = 0\n",
    "\n",
    "# Function to make an empty mask of the same size as the model\n",
    "def make_mask(model):\n",
    "    global step\n",
    "    global mask\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            step = step + 1\n",
    "    mask = [None]* step \n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            tensor = param.data.cpu().numpy()\n",
    "            mask[step] = np.ones_like(tensor)\n",
    "            step = step + 1\n",
    "    step = 0\n",
    "\n",
    "def original_initialization(mask_temp, initial_state_dict):\n",
    "    global model\n",
    "    \n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if \"weight\" in name: \n",
    "            weight_dev = param.device\n",
    "            param.data = torch.from_numpy(mask_temp[step] * initial_state_dict[name].cpu().numpy()).to(weight_dev)\n",
    "            step = step + 1\n",
    "        if \"bias\" in name:\n",
    "            param.data = initial_state_dict[name]\n",
    "    step = 0\n",
    "\n",
    "# Function for Initialization\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    #from gooey import Gooey\n",
    "    #@Gooey      \n",
    "    \n",
    "    # Arguement Parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--lr\",default= 1.2e-3, type=float, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", default=60, type=int)\n",
    "    parser.add_argument(\"--start_iter\", default=0, type=int)\n",
    "    parser.add_argument(\"--end_iter\", default=100, type=int)\n",
    "    parser.add_argument(\"--print_freq\", default=1, type=int)\n",
    "    parser.add_argument(\"--valid_freq\", default=1, type=int)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\")\n",
    "    parser.add_argument(\"--prune_type\", default=\"lt\", type=str, help=\"lt | reinit\")\n",
    "    parser.add_argument(\"--gpu\", default=\"0\", type=str)\n",
    "    parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10 | fashionmnist | cifar100\")\n",
    "    parser.add_argument(\"--arch_type\", default=\"fc1\", type=str, help=\"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\")\n",
    "    parser.add_argument(\"--prune_percent\", default=10, type=int, help=\"Pruning percent\")\n",
    "    parser.add_argument(\"--prune_iterations\", default=35, type=int, help=\"Pruning iterations count\")\n",
    "\n",
    "    lr = 1.2e-3\n",
    "    batch_size = 60\n",
    "    start_iter = 0\n",
    "    end_iter = 100\n",
    "    print_freq = 1\n",
    "    valid_freq = 1\n",
    "    resume = \"store_true\"\n",
    "    prune_type = lt #reinit\n",
    "    gpu = \"0\"\n",
    "    dataset = \"mnist\" #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "    arch_type = \"fc1\" # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "    prune_percent  = 10 \n",
    "    prune_iterations = 35 \n",
    "        \n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n",
    "    \n",
    "    \n",
    "    #FIXME resample\n",
    "    resample = False\n",
    "\n",
    "    # Looping Entire process\n",
    "    #for i in range(0, 5):\n",
    "    main(args, ITE=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class args(self):\n",
    "        self.r = realpart\n",
    "        self.i = imagpart\n",
    "        self.    lr = 1.2e-3\n",
    "        self.batch_size = 60\n",
    "        self.start_iter = 0\n",
    "        self.end_iter = 100\n",
    "        self.print_freq = 1\n",
    "        self.valid_freq = 1\n",
    "        self.resume = \"store_true\"\n",
    "        self.prune_type = lt #reinit\n",
    "        self.gpu = \"0\"\n",
    "        self.dataset = \"mnist\" #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "        self.arch_type = \"fc1\" # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "        self.prune_percent  = 10 \n",
    "        self.prune_iterations = 35 \n",
    "    \n",
    "    def __init__(self, realpart, imagpart):\n",
    "        self.r = realpart\n",
    "        self.i = imagpart\n",
    "        self.    lr = 1.2e-3\n",
    "        self.batch_size = 60\n",
    "        self.start_iter = 0\n",
    "        self.end_iter = 100\n",
    "        self.print_freq = 1\n",
    "        self.valid_freq = 1\n",
    "        self.resume = \"store_true\"\n",
    "        self.prune_type = lt #reinit\n",
    "        self.gpu = \"0\"\n",
    "        self.dataset = \"mnist\" #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "        self.arch_type = \"fc1\" # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "        self.prune_percent  = 10 \n",
    "        self.prune_iterations = 35 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                             [--start_iter START_ITER] [--end_iter END_ITER]\n",
      "                             [--print_freq PRINT_FREQ]\n",
      "                             [--valid_freq VALID_FREQ] [--resume]\n",
      "                             [--prune_type PRUNE_TYPE] [--gpu GPU]\n",
      "                             [--dataset DATASET] [--arch_type ARCH_TYPE]\n",
      "                             [--prune_percent PRUNE_PERCENT]\n",
      "                             [--prune_iterations PRUNE_ITERATIONS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jovyan/.local/share/jupyter/runtime/kernel-989eab33-b36f-4f7e-b775-c07883dbba9e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default= 1.2e-3, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\"--batch_size\", default=60, type=int)\n",
    "parser.add_argument(\"--start_iter\", default=0, type=int)\n",
    "parser.add_argument(\"--end_iter\", default=100, type=int)\n",
    "parser.add_argument(\"--print_freq\", default=1, type=int)\n",
    "parser.add_argument(\"--valid_freq\", default=1, type=int)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\")\n",
    "parser.add_argument(\"--prune_type\", default=\"lt\", type=str, help=\"lt | reinit\")\n",
    "parser.add_argument(\"--gpu\", default=\"0\", type=str)\n",
    "parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10 | fashionmnist | cifar100\")\n",
    "parser.add_argument(\"--arch_type\", default=\"fc1\", type=str, help=\"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\")\n",
    "parser.add_argument(\"--prune_percent\", default=10, type=int, help=\"Pruning percent\")\n",
    "parser.add_argument(\"--prune_iterations\", default=35, type=int, help=\"Pruning iterations count\")\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
