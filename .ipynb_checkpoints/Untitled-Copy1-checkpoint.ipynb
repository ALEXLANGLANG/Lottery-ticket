{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import seaborn as sns\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "\n",
    "# Custom Libraries\n",
    "import utils\n",
    "# Tensorboard initialization\n",
    "# writer = SummaryWriter()\n",
    "# Plotting Style\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Function for Training\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #imgs, targets = next(train_loader)\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Freezing Pruned weights by making their gradients Zero\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                p.grad.data = torch.where(p.data < EPS,torch.tensor(0).to(device), p.grad.data)\n",
    "#                 tensor = p.data.cpu().numpy()\n",
    "#                 grad_tensor = p.grad.data.cpu().numpy()\n",
    "#                 grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "#                 p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\n",
    "        optimizer.step()\n",
    "    return train_loss.item()\n",
    "\n",
    "# Function for Testing\n",
    "def test(model, test_loader, criterion):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "# Prune by Percentile module\n",
    "def prune_by_percentile(percent, resample=False, reinit=False,**kwargs):\n",
    "        global step\n",
    "        global mask\n",
    "        global model\n",
    "\n",
    "        # Calculate percentile value\n",
    "        step = 0\n",
    "        for name, param in model.named_parameters():\n",
    "\n",
    "            # We do not prune bias term\n",
    "            if 'weight' in name:\n",
    "                tensor = param.data.cpu().numpy()\n",
    "                alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
    "                percentile_value = np.percentile(abs(alive), percent)\n",
    "\n",
    "                # Convert Tensors to numpy and calculate\n",
    "                weight_dev = param.device\n",
    "                new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])\n",
    "                \n",
    "                # Apply new weight and mask\n",
    "                param.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "#                 tensor = param.data\n",
    "#                 percentile_value = np.percentile(abs(tensor[torch.nonzero(tensor)].cpu().numpy()), percent)\n",
    "#                 new_mask = torch.where(torch.abs(tensor) < percentile_value, 0, mask[step])  \n",
    "#                 param.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "                     \n",
    "                mask[step] = new_mask\n",
    "                step += 1\n",
    "        step = 0\n",
    "\n",
    "# Function to make an empty mask of the same size as the model\n",
    "def make_mask(model):\n",
    "    global step\n",
    "    global mask\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            step = step + 1\n",
    "    mask = [None]* step \n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            tensor = param.data.cpu().numpy()\n",
    "            mask[step] = np.ones_like(tensor)\n",
    "            step = step + 1\n",
    "    step = 0\n",
    "\n",
    "def original_initialization(mask_temp, initial_state_dict):\n",
    "    global model\n",
    "    \n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if \"weight\" in name: \n",
    "            weight_dev = param.device\n",
    "            param.data = torch.from_numpy(mask_temp[step] * initial_state_dict[name].cpu().numpy()).to(weight_dev)\n",
    "            step = step + 1\n",
    "        if \"bias\" in name:\n",
    "            param.data = initial_state_dict[name]\n",
    "    step = 0\n",
    "\n",
    "# Function for Initialization\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    #from gooey import Gooey\n",
    "    #@Gooey      \n",
    "    \n",
    "    # Arguement Parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--lr\",default= 1.2e-3, type=float, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", default=60, type=int)\n",
    "    parser.add_argument(\"--start_iter\", default=0, type=int)\n",
    "    parser.add_argument(\"--end_iter\", default=100, type=int)\n",
    "    parser.add_argument(\"--print_freq\", default=1, type=int)\n",
    "    parser.add_argument(\"--valid_freq\", default=1, type=int)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\")\n",
    "    parser.add_argument(\"--prune_type\", default=\"lt\", type=str, help=\"lt | reinit\")\n",
    "    parser.add_argument(\"--gpu\", default=\"0\", type=str)\n",
    "    parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10 | fashionmnist | cifar100\")\n",
    "    parser.add_argument(\"--arch_type\", default=\"fc1\", type=str, help=\"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\")\n",
    "    parser.add_argument(\"--prune_percent\", default=10, type=int, help=\"Pruning percent\")\n",
    "    parser.add_argument(\"--prune_iterations\", default=35, type=int, help=\"Pruning iterations count\")\n",
    "\n",
    "#     lr = 1.2e-3\n",
    "#     batch_size = 60\n",
    "#     start_iter = 0\n",
    "#     end_iter = 100\n",
    "#     print_freq = 1\n",
    "#     valid_freq = 1\n",
    "#     resume = \"store_true\"\n",
    "#     prune_type = lt #reinit\n",
    "#     gpu = \"0\"\n",
    "#     dataset = \"mnist\" #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "#     arch_type = \"fc1\" # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "#     prune_percent  = 10 \n",
    "#     prune_iterations = 35 \n",
    "        \n",
    "\n",
    "#     os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n",
    "    \n",
    "    \n",
    "    #FIXME resample\n",
    "#     resample = False\n",
    "\n",
    "    # Loopin=1)g Entire process\n",
    "    #for i in range(0, 5):\n",
    "#     main(args, ITE)\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "def main(args, ITE=0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    reinit = True if args.prune_type==\"reinit\" else False\n",
    "\n",
    "    # Data Loader\n",
    "    transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    if args.dataset == \"mnist\":\n",
    "        traindataset = datasets.MNIST('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.MNIST('~/work/data/Xian', train=False, transform=transform)\n",
    "        from archs.mnist import AlexNet, LeNet5, fc1, vgg, resnet\n",
    "\n",
    "    elif args.dataset == \"cifar10\":\n",
    "        traindataset = datasets.CIFAR10('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.CIFAR10('~/work/data/Xian', train=False, transform=transform)      \n",
    "        from archs.cifar10 import AlexNet, LeNet5, fc1, vgg, resnet, densenet \n",
    "\n",
    "    elif args.dataset == \"fashionmnist\":\n",
    "        traindataset = datasets.FashionMNIST('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.FashionMNIST('~/work/data/Xian', train=False, transform=transform)\n",
    "        from archs.mnist import AlexNet, LeNet5, fc1, vgg, resnet \n",
    "\n",
    "    elif args.dataset == \"cifar100\":\n",
    "        traindataset = datasets.CIFAR100('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "        testdataset = datasets.CIFAR100('~/work/data/Xian', train=False, transform=transform)   \n",
    "        from archs.cifar100 import AlexNet, fc1, LeNet5, vgg, resnet  \n",
    "    \n",
    "    # If you want to add extra datasets paste here\n",
    "\n",
    "    else:\n",
    "        print(\"\\nWrong Dataset choice \\n\")\n",
    "        exit()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(traindataset, batch_size=args.batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "    #train_loader = cycle(train_loader)\n",
    "    test_loader = torch.utils.data.DataLoader(testdataset, batch_size=args.batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
    "    \n",
    "    # Importing Network Architecture\n",
    "    global model\n",
    "    if args.arch_type == \"fc1\":\n",
    "        model = fc1.fc1().to(device)\n",
    "    elif args.arch_type == \"lenet5\":\n",
    "        model = LeNet5.LeNet5().to(device)\n",
    "    elif args.arch_type == \"alexnet\":\n",
    "        model = AlexNet.AlexNet().to(device)\n",
    "    elif args.arch_type == \"vgg16\":\n",
    "        model = vgg.vgg16().to(device)  \n",
    "    elif args.arch_type == \"resnet18\":\n",
    "        model = resnet.resnet18().to(device)   \n",
    "    elif args.arch_type == \"densenet121\":\n",
    "        model = densenet.densenet121().to(device)   \n",
    "    # If you want to add extra model paste here\n",
    "    else:\n",
    "        print(\"\\nWrong Model choice\\n\")\n",
    "        exit()\n",
    "\n",
    "    # Weight Initialization\n",
    "    model.apply(weight_init)\n",
    "\n",
    "    # Copying and Saving Initial State\n",
    "    initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "    utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "    torch.save(model, f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/initial_state_dict_{args.prune_type}.pth.tar\")\n",
    "\n",
    "    # Making Initial Mask\n",
    "    make_mask(model)\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss() # Default was F.nll_loss\n",
    "\n",
    "    # Layer Looper\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.size())\n",
    "\n",
    "    # Pruning\n",
    "    # NOTE First Pruning Iteration is of No Compression\n",
    "    bestacc = 0.0\n",
    "    best_accuracy = 0\n",
    "    ITERATION = args.prune_iterations\n",
    "    comp = np.zeros(ITERATION,float)\n",
    "    bestacc = np.zeros(ITERATION,float)\n",
    "    step = 0\n",
    "    all_loss = np.zeros(args.end_iter,float)\n",
    "    all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "\n",
    "    for _ite in range(args.start_iter, ITERATION):\n",
    "        if not _ite == 0:\n",
    "            prune_by_percentile(args.prune_percent, resample=resample, reinit=reinit)\n",
    "            if reinit:\n",
    "                model.apply(weight_init)\n",
    "                step = 0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        weight_dev = param.device\n",
    "                        param.data = torch.from_numpy(param.data.cpu().numpy() * mask[step]).to(weight_dev)\n",
    "                        step = step + 1\n",
    "                step = 0\n",
    "            else:\n",
    "                original_initialization(mask, initial_state_dict)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "        print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "        # Print the table of Nonzeros in each layer\n",
    "        comp1 = utils.print_nonzeros(model)\n",
    "        comp[_ite] = comp1\n",
    "        pbar = tqdm(range(args.end_iter))\n",
    "\n",
    "        for iter_ in pbar:\n",
    "\n",
    "            # Frequency for Testing\n",
    "            if iter_ % args.valid_freq == 0:\n",
    "                accuracy = test(model, test_loader, criterion)\n",
    "\n",
    "                # Save Weights\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "                    torch.save(model,f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/{_ite}_model_{args.prune_type}.pth.tar\")\n",
    "\n",
    "            # Training\n",
    "            loss = train(model, train_loader, optimizer, criterion)\n",
    "            all_loss[iter_] = loss\n",
    "            all_accuracy[iter_] = accuracy\n",
    "            \n",
    "            # Frequency for Printing Accuracy and Loss\n",
    "            if iter_ % args.print_freq == 0:\n",
    "                pbar.set_description(\n",
    "                    f'Train Epoch: {iter_}/{args.end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "        writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "        bestacc[_ite]=best_accuracy\n",
    "\n",
    "        # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "        #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "        #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "        plt.plot(np.arange(1,(args.end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "        plt.plot(np.arange(1,(args.end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "        plt.title(f\"Loss Vs Accuracy Vs Iterations ({args.dataset},{args.arch_type})\") \n",
    "        plt.xlabel(\"Iterations\") \n",
    "        plt.ylabel(\"Loss and Accuracy\") \n",
    "        plt.legend() \n",
    "        plt.grid(color=\"gray\") \n",
    "        utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "        plt.close()\n",
    "\n",
    "        # Dump Plot values\n",
    "        utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        all_loss.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_loss_{comp1}.dat\")\n",
    "        all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_accuracy_{comp1}.dat\")\n",
    "        \n",
    "        # Dumping mask\n",
    "        utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "        with open(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "            pickle.dump(mask, fp)\n",
    "        \n",
    "        # Making variables into 0\n",
    "        best_accuracy = 0\n",
    "        all_loss = np.zeros(args.end_iter,float)\n",
    "        all_accuracy = np.zeros(args.end_iter,float)\n",
    "\n",
    "    # Dumping Values for Plotting\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    comp.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_compression.dat\")\n",
    "    bestacc.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_bestaccuracy.dat\")\n",
    "\n",
    "    # Plotting\n",
    "    a = np.arange(args.prune_iterations)\n",
    "    plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "    plt.title(f\"Test Accuracy vs Unpruned Weights Percentage ({args.dataset},{args.arch_type})\") \n",
    "    plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "    plt.ylabel(\"test accuracy\") \n",
    "    plt.xticks(a, comp, rotation =\"vertical\") \n",
    "    plt.ylim(0,100)\n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_AccuracyVsWeights.png\", dpi=1200) \n",
    "    plt.close()                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(t: torch.tensor, q: float):\n",
    "    \"\"\"\n",
    "    Return the ``q``-th percentile of the flattened input tensor's data.\n",
    "    \n",
    "    CAUTION:\n",
    "     * Needs PyTorch >= 1.1.0, as ``torch.kthvalue()`` is used.\n",
    "     * Values are not interpolated, which corresponds to\n",
    "       ``numpy.percentile(..., interpolation=\"nearest\")``.\n",
    "       \n",
    "    :param t: Input tensor.\n",
    "    :param q: Percentile to compute, which must be between 0 and 100 inclusive.\n",
    "    :return: Resulting value (scalar).\n",
    "    \"\"\"\n",
    "    # Note that ``kthvalue()`` works one-based, i.e. the first sorted value\n",
    "    # indeed corresponds to k=1, not k=0! Use float(q) instead of q directly,\n",
    "    # so that ``round()`` returns an integer, even if q is a np.float32.\n",
    "    k = 1 + round(.01 * float(q) * (t.numel() - 1))\n",
    "    result = t.view(-1).kthvalue(k).values.item()\n",
    "    return result\n",
    "\n",
    "class argument:\n",
    "    def __init__(self, lr=1.2e-3,batch_size = 60,start_iter = 0,end_iter = 100,print_freq = 1,\n",
    "                 valid_freq = 1,resume = \"store_true\",prune_type= \"lt\",gpu = \"0\",\n",
    "                 dataset = \"mnist\" ,arch_type = \"fc1\",prune_percent  = 10,prune_iterations = 35):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.start_iter = start_iter\n",
    "        self.end_iter = end_iter\n",
    "        self.print_freq = print_freq\n",
    "        self.valid_freq = valid_freq\n",
    "        self.resume = \"store_true\"\n",
    "        self.prune_type = prune_type #reinit\n",
    "        self.gpu = \"0\"\n",
    "        self.dataset = \"mnist\" #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "        self.arch_type = \"fc1\" # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "        self.prune_percent  = 10 \n",
    "        self.prune_iterations = 35 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Training\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #imgs, targets = next(train_loader)\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Freezing Pruned weights by making their gradients Zero\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "#                 print(torch.tensor(0,dtype = torch.float).to(device).type())\n",
    "#                 print(p.grad.data.type())\n",
    "                p.grad.data = torch.where(p.data < EPS,torch.tensor(0,dtype = torch.float).to(device), p.grad.data)\n",
    "#                 tensor = p.data.cpu().numpy()\n",
    "#                 grad_tensor = p.grad.data.cpu().numpy()\n",
    "#                 grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "#                 p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\n",
    "        optimizer.step()\n",
    "    return train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argument(end_iter = 50)\n",
    "resample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ~work/data/Xian/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed5e6a1587d42fabf9cf124c02eb701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ~work/data/Xian/MNIST/raw/train-images-idx3-ubyte.gz to ~work/data/Xian/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ~work/data/Xian/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1420d3323814ffca20eb0727f1162a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ~work/data/Xian/MNIST/raw/train-labels-idx1-ubyte.gz to ~work/data/Xian/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ~work/data/Xian/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356b9b9c6f0b4f61a29a7d7c4d6b161f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ~work/data/Xian/MNIST/raw/t10k-images-idx3-ubyte.gz to ~work/data/Xian/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ~work/data/Xian/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a1370cf27b4de98906a3cf66168c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ~work/data/Xian/MNIST/raw/t10k-labels-idx1-ubyte.gz to ~work/data/Xian/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.linear.weight torch.Size([300, 784])\n",
      "classifier.0.linear.bias torch.Size([300])\n",
      "classifier.2.linear.weight torch.Size([100, 300])\n",
      "classifier.2.linear.bias torch.Size([100])\n",
      "classifier.4.linear.weight torch.Size([10, 100])\n",
      "classifier.4.linear.bias torch.Size([10])\n",
      "\n",
      "--- Pruning Level [0:0/35]: ---\n",
      "classifier.0.linear.weight | nonzeros =  235200 /  235200 (100.00%) | total_pruned =       0 | shape = (300, 784)\n",
      "classifier.0.linear.bias | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "classifier.2.linear.weight | nonzeros =   30000 /   30000 (100.00%) | total_pruned =       0 | shape = (100, 300)\n",
      "classifier.2.linear.bias | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "classifier.4.linear.weight | nonzeros =    1000 /    1000 (100.00%) | total_pruned =       0 | shape = (10, 100)\n",
      "classifier.4.linear.bias | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 266610, pruned : 0, total: 266610, Compression rate :       1.00x  (  0.00% pruned)\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
