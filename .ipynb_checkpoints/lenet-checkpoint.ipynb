{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "from prune_layer import *\n",
    "\n",
    "\n",
    "# Custom Libraries\n",
    "import utils\n",
    "class argument:\n",
    "    def __init__(self, lr=1.2e-3,batch_size = 60,start_iter = 0,end_iter = 100,print_freq = 1,\n",
    "                 valid_freq = 1,resume = \"store_true\",prune_type= \"lt\",gpu = \"0\",\n",
    "                 dataset = \"mnist\" ,arch_type = \"fc1\",prune_percent  = 10,prune_iterations = 35):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.start_iter = start_iter\n",
    "        self.end_iter = end_iter\n",
    "        self.print_freq = print_freq\n",
    "        self.valid_freq = valid_freq\n",
    "        self.resume = resume\n",
    "        self.prune_type = prune_type #reinit\n",
    "        self.gpu = gpu\n",
    "        self.dataset = dataset #\"mnist | cifar10 | fashionmnist | cifar100\"\n",
    "        self.arch_type = arch_type # \"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\"\n",
    "        self.prune_percent  = prune_percent \n",
    "        self.prune_iterations = prune_iterations \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.conv.weight torch.Size([64, 1, 3, 3])\n",
      "conv1.conv.bias torch.Size([64])\n",
      "BN2d_1.weight torch.Size([64])\n",
      "BN2d_1.bias torch.Size([64])\n",
      "conv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "conv2.conv.bias torch.Size([64])\n",
      "BN2d_2.weight torch.Size([64])\n",
      "BN2d_2.bias torch.Size([64])\n",
      "fc1.linear.weight torch.Size([256, 12544])\n",
      "fc1.linear.bias torch.Size([256])\n",
      "BN1d_1.weight torch.Size([256])\n",
      "BN1d_1.bias torch.Size([256])\n",
      "fc2.linear.weight torch.Size([256, 256])\n",
      "fc2.linear.bias torch.Size([256])\n",
      "BN1d_2.weight torch.Size([256])\n",
      "BN1d_2.bias torch.Size([256])\n",
      "fc3.linear.weight torch.Size([10, 256])\n",
      "fc3.linear.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "args = argument(end_iter = 50,arch_type =\"lenet5\")\n",
    "reinit = True if args.prune_type==\"reinit\" else False\n",
    "\n",
    "#Data Loader\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "if args.dataset == \"mnist\":\n",
    "    traindataset = datasets.MNIST('~/work/data/Xian', train=True, download=True,transform=transform)\n",
    "    testdataset = datasets.MNIST('~/work/data/Xian', train=False, transform=transform)\n",
    "    from archs.mnist import  LeNet5, fc1, vgg, resnet,AlexNet\n",
    "# If you want to add extra datasets paste here\n",
    "else:\n",
    "    print(\"\\nWrong Dataset choice \\n\")\n",
    "    exit()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(traindataset, batch_size=args.batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size=args.batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
    "\n",
    "# Importing Network Architecture\n",
    "global model\n",
    "if args.arch_type == \"fc1\":\n",
    "    model = fc1.fc1().to(device)\n",
    "elif args.arch_type == \"lenet5\":\n",
    "    model = LeNet5.LeNet5().to(device)\n",
    "else:\n",
    "    print(\"\\nWrong Model choice\\n\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Copying and Saving Initial State\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "torch.save(model, f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/initial_state_dict_{args.prune_type}.pth.tar\")\n",
    "\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss() # Default was F.nll_loss\n",
    "\n",
    "# Layer Looper\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "    \n",
    "    \n",
    "# Pruning\n",
    "# NOTE First Pruning Iteration is of No Compression\n",
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "ITERATION = args.prune_iterations\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(args.end_iter,float)\n",
    "all_accuracy = np.zeros(args.end_iter,float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_percentage_nonzero(q = 10):\n",
    "    global model \n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            m.prune_by_percentage(q = q)\n",
    "        if isinstance(m, PruneLinear):\n",
    "            m.prune_by_percentage(q = q)\n",
    "            \n",
    "def mask_weights(mask_data = True): \n",
    "    global model \n",
    "    if mask_data:\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, PrunedConv):\n",
    "                m.conv.weight.data.mul_(m.mask)\n",
    "            if isinstance(m, PruneLinear):\n",
    "                m.linear.weight.data.mul_(m.mask)\n",
    "    else:\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, PrunedConv):\n",
    "                m.conv.weight.grad.mul_(m.mask)\n",
    "            if isinstance(m, PruneLinear):\n",
    "                m.linear.weight.grad.mul_(m.mask)\n",
    "            \n",
    "def initialize_weights(initial_state_dict):\n",
    "    global model \n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, PrunedConv):\n",
    "            m.conv.weight.data = m.mask*initial_state_dict[n + '.conv.weight']\n",
    "            m.conv.weight.bias = initial_state_dict[n + '.conv.bias']\n",
    "        if isinstance(m, PruneLinear):\n",
    "            m.linear.weight.data = m.mask*initial_state_dict[n + '.linear.weight']\n",
    "            m.linear.weight.bias =initial_state_dict[n + '.linear.bias']\n",
    "            \n",
    "def reintilize_weights():\n",
    "    global model \n",
    "    model.apply(weight_init)\n",
    "    mask_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_dict = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Training\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask_weights()#Mask data into zero\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "\n",
    "        mask_weights(False) #Mask gradients of weights to zero\n",
    "        optimizer.step()\n",
    "        \n",
    "    return train_loss.item()\n",
    "\n",
    "# Function for Testing\n",
    "def test(model, test_loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
